{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49da22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"max_seq_items\", None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "import pipes\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from rapidfuzz import process, fuzz\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89435633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f672ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('data/train.pkl','rb'))\n",
    "test = pickle.load(open('data/test.pkl','rb'))\n",
    "preprocess_pipeline = pickle.load(open('data/pipeline.pkl', 'rb'))\n",
    "\n",
    "train = preprocess_pipeline.fit_transform(train)\n",
    "test = preprocess_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9eecc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7930, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f233327",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('pressure',axis=1).values\n",
    "y = train['pressure'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787ac09",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560bd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[100]):\n",
    "        \"\"\"\n",
    "        - input_dim: number of neurons on input layer\n",
    "        - hidden_layers: list of integers where each integer represents\n",
    "                         the number of neurons in that hidden layer\n",
    "        (ie. [100, 200]: input_dim -> 100 -> relu -> 200 -> relu -> output)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim,h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "          \n",
    "        layers.append(nn.Linear(in_dim,1))    # output layer\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36280dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildMLP(input_dim, hidden_layers):\n",
    "    return MLP(input_dim, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01b1ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP(model, X, y, criterion, optimizer, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - optimizer: parameter optimizer algorithm\n",
    "    - batch_size: batch size for mini-batch gradient descent\n",
    "    - num_epochs: number of epochs to train\n",
    "    \n",
    "    Output:\n",
    "    - model: trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            print(f'Training epoch {epoch+1}/{num_epochs}')\n",
    "        model.train()\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMLP(model, X, y, criterion, batch_size=32):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - batch_size: data loader batch size (not very important, only lightens the load for loss each calculation)\n",
    "    \n",
    "    Output:\n",
    "    - mean_loss: mean loss from all tested samples\n",
    "    \"\"\"\n",
    "        \n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item() * batch_X.size(0)\n",
    "            total_samples += batch_X.size(0)\n",
    "            \n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1656b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(X, y, criterion, optimizer_class, optimizer_kwargs, \n",
    "                  hidden_layers=[100], num_folds=5, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss functions\n",
    "    - optimizer_class: gradient optimisation\n",
    "    - optimizer_kwargs: learning rate as input\n",
    "    - hidden_layers: list of integers where each integer represents\n",
    "                     the number of neurons in that hidden layer\n",
    "    - num_folds: number of cross validation folds\n",
    "    - batch_size: batch size for DataLoader (mini-batch gradient descent)\n",
    "    - num_epochs: number of epochs to train per fold\n",
    "    \n",
    "    Output\n",
    "    - fold_train_loss: list of training losses for each fold\n",
    "    - fold_test_loss: list of testing losses for each fold\n",
    "    \"\"\"\n",
    "        \n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_test_loss = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        print(f'\\nFold {fold+1}/{num_folds}')\n",
    "        \n",
    "        # Create new model and optimizer for each fold\n",
    "        model = buildMLP(X.shape[1], hidden_layers)\n",
    "        optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train with current fold's data\n",
    "        model = trainMLP(model, X_train, y_train, criterion, optimizer,\n",
    "                        batch_size, num_epochs)\n",
    "        \n",
    "        # Evaluate on both train and test sets\n",
    "        train_loss = evalMLP(model, X_train, y_train, criterion)\n",
    "        test_loss = evalMLP(model, X_test, y_test, criterion)\n",
    "        \n",
    "        fold_train_loss.append(train_loss)\n",
    "        fold_test_loss.append(test_loss)\n",
    "        \n",
    "        print(f'Fold {fold+1} - Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "    \n",
    "    return fold_train_loss, fold_test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef60bbb",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42a7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e944d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = buildMLP(X.shape[1], [100,200])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28779d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_kwargs = {'lr': 0.0001}\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9fd2c",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71a42d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0890,  0.0072, -0.1084,  ..., -0.0515,  0.0700,  0.1543],\n",
      "        [ 0.0407,  0.0597,  0.2106,  ...,  0.0144, -0.0407, -0.1170],\n",
      "        [-0.0198,  0.2067, -0.0172,  ...,  0.0344,  0.1803, -0.2062],\n",
      "        ...,\n",
      "        [ 0.0322,  0.1624, -0.0922,  ..., -0.0731, -0.0767, -0.1934],\n",
      "        [ 0.0461,  0.1464, -0.1214,  ...,  0.0707, -0.1888, -0.0771],\n",
      "        [ 0.1751, -0.1484, -0.1763,  ..., -0.1771,  0.0013,  0.1413]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Finding the initial weights \n",
    "print(model.layer[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d6b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n"
     ]
    }
   ],
   "source": [
    "model = trainMLP(model, X, y, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08254573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1044,  0.0422, -0.1435,  ..., -0.1271,  0.2082,  0.2289],\n",
      "        [ 0.0812,  0.0372,  0.1736,  ..., -0.1752, -0.0095, -0.0008],\n",
      "        [-0.0611,  0.2747, -0.0505,  ...,  0.0721,  0.2734, -0.1974],\n",
      "        ...,\n",
      "        [ 0.0335,  0.2382, -0.0788,  ...,  0.0092, -0.1270, -0.1845],\n",
      "        [ 0.0539,  0.1625, -0.0052,  ...,  0.0854, -0.0151, -0.0970],\n",
      "        [ 0.1674, -0.1501, -0.1517,  ..., -0.2085, -0.0396,  0.2282]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Updates weights to first hidden layer after training the model\n",
    "print(model.layer[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff1d4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evalMLP(model, X, y, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2b582e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008619799806230231\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b779635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n",
      "Fold 1 - Train loss: 0.0091, Test loss: 0.0291\n",
      "\n",
      "Fold 2/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n",
      "Fold 2 - Train loss: 0.0084, Test loss: 0.0273\n",
      "\n",
      "Fold 3/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n",
      "Fold 3 - Train loss: 0.0095, Test loss: 0.0289\n",
      "\n",
      "Fold 4/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n",
      "Fold 4 - Train loss: 0.0087, Test loss: 0.0306\n",
      "\n",
      "Fold 5/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n",
      "Fold 5 - Train loss: 0.0105, Test loss: 0.0308\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = crossvalidate(X, y, criterion, optimizer_class, optimizer_kwargs, hidden_layers=[100,200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47cb8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.009103134811196997, 0.008402398863776785, 0.009535001671883602, 0.008745870433165251, 0.010464281365181429]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f96013af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.029074125628434543, 0.027287184023550827, 0.028891845842327776, 0.03060407891005165, 0.030849941845273912]\n"
     ]
    }
   ],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25305418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
