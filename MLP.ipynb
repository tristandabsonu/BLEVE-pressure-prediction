{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510a5057",
   "metadata": {},
   "source": [
    "This notebook will be the first one for training an MLP. I will create functions for training, testing, crossvalidation and randomsearch to have a seamless analysis. The baseline default value I will use for the initial MLP will come from sklearn's MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2d77d",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "<br />\n",
    "class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49da22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"max_seq_items\", None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "import numpy as np\n",
    "\n",
    "# import pipes\n",
    "# from sklearn.impute import KNNImputer\n",
    "# from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from rapidfuzz import process, fuzz\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89435633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f672ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('data/train.pkl','rb'))\n",
    "test = pickle.load(open('data/test.pkl','rb'))\n",
    "preprocess_pipeline = pickle.load(open('data/pipeline.pkl', 'rb'))\n",
    "\n",
    "train = preprocess_pipeline.fit_transform(train)\n",
    "test = preprocess_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f9eecc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7930, 22)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f233327",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('pressure',axis=1).values\n",
    "y = train['pressure'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787ac09",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "560bd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=[100], dropout_prob=0):\n",
    "        \"\"\"\n",
    "        - input_dim: number of neurons on input layer\n",
    "        - hidden_layers: list of integers where each integer represents\n",
    "                         the number of neurons in that hidden layer\n",
    "        (ie. [100, 200]: input_dim -> 100 -> relu -> 200 -> relu -> output)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim,h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_prob))    # dropout regularisation\n",
    "            in_dim = h\n",
    "          \n",
    "        layers.append(nn.Linear(in_dim,1))    # output layer\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "500782b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.count = 0\n",
    "\n",
    "    def earlystop(self, val_loss):\n",
    "        # Significant decrease in validation loss\n",
    "        if (self.best_val_loss - val_loss) > self.delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.count = 0\n",
    "\n",
    "        # Insignificant decrease in validation loss\n",
    "        elif (self.best_val_loss - val_loss) <= self.delta:\n",
    "            self.count += 1\n",
    "            if self.count > self.patience:\n",
    "                return True\n",
    "            \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01b1ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP(model, X, y, criterion, optimizer, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - optimizer: parameter optimizer algorithm\n",
    "    - batch_size: batch size for mini-batch gradient descent\n",
    "    - num_epochs: number of epochs to train\n",
    "    \n",
    "    Output:\n",
    "    - model: trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            print(f'Training epoch {epoch+1}/{num_epochs}')\n",
    "            \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backwardprop and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "636426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMLP(model, X, y, criterion, batch_size=32):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - batch_size: data loader batch size (not very important, only lightens the load for loss each calculation)\n",
    "    \n",
    "    Output:\n",
    "    - mean_loss: mean loss from all tested samples\n",
    "    \"\"\"\n",
    "        \n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item() * batch_X.size(0)\n",
    "            total_samples += batch_X.size(0)\n",
    "            \n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da270825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP_earlystop(model, X, y, criterion, optimizer, patience=15, delta= 0, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - optimizer: parameter optimizer algorithm\n",
    "    - patience: number of consecutive insignificant decreases in validation loss until early stop to activate\n",
    "    - delta: the magnitude a decrease needs to be significant (val_loss new - val_loss old) \n",
    "    - batch_size: batch size for mini-batch gradient descent\n",
    "    - num_epochs: number of epochs to train\n",
    "    \n",
    "    Output:\n",
    "    - model: trained model\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    es = EarlyStopper(patience, delta)\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        if (epoch+1) % 20 == 0:\n",
    "            print(f'Training epoch {epoch+1}/{num_epochs}')\n",
    "            \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # early stopping check\n",
    "        val_loss = evalMLP(model, X_val, y_val, criterion, batch_size)\n",
    "        if es.earlystop(val_loss):\n",
    "            # Break epoch training loop if earlystop return True\n",
    "            break\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1656b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(X, y, criterion, optimizer_class, optimizer_kwargs, patience=15, delta= 0,\n",
    "                  hidden_layers=[100], dropout_prob=0, num_folds=5, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss functions\n",
    "    - optimizer_class: gradient optimisation\n",
    "    - optimizer_kwargs: learning rate as input\n",
    "    - patience: number of consecutive insignificant decreases in validation loss until early stop to activate\n",
    "    - delta: the magnitude a decrease needs to be significant (val_loss new - val_loss old) \n",
    "    - hidden_layers: list of integers where each integer represents\n",
    "                     the number of neurons in that hidden layer\n",
    "    - dropout_prob: probability of neurons dropping out\n",
    "    - num_folds: number of cross validation folds\n",
    "    - batch_size: batch size for DataLoader (mini-batch gradient descent)\n",
    "    - num_epochs: number of epochs to train per fold\n",
    "    \n",
    "    Output\n",
    "    - fold_train_loss: list of training losses for each fold\n",
    "    - fold_test_loss: list of testing losses for each fold\n",
    "    \"\"\"\n",
    "        \n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_test_loss = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        print(f'\\nFold {fold+1}/{num_folds}')\n",
    "        \n",
    "        # Create new model and optimizer for each fold\n",
    "        model = MLP(X.shape[1], hidden_layers, dropout_prob)\n",
    "        optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train with current fold's data\n",
    "        model = trainMLP_earlystop(model, X_train, y_train, criterion, optimizer,\n",
    "                                   patience, delta, batch_size, num_epochs)\n",
    "        \n",
    "        # Evaluate on both train and test sets\n",
    "        train_loss = evalMLP(model, X_train, y_train, criterion)\n",
    "        test_loss = evalMLP(model, X_test, y_test, criterion)\n",
    "        \n",
    "        fold_train_loss.append(train_loss)\n",
    "        fold_test_loss.append(test_loss)\n",
    "        \n",
    "        print(f'Fold {fold+1} - Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "    \n",
    "    return fold_train_loss, fold_test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef60bbb",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c42a7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e944d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0, inplace=False)\n",
      "    (6): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], [100,200])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28779d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_kwargs = {'lr': 0.0001}\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd9fd2c",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "71a42d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0890,  0.0072, -0.1084,  ..., -0.0515,  0.0700,  0.1543],\n",
      "        [ 0.0407,  0.0597,  0.2106,  ...,  0.0144, -0.0407, -0.1170],\n",
      "        [-0.0198,  0.2067, -0.0172,  ...,  0.0344,  0.1803, -0.2062],\n",
      "        ...,\n",
      "        [ 0.0322,  0.1624, -0.0922,  ..., -0.0731, -0.0767, -0.1934],\n",
      "        [ 0.0461,  0.1464, -0.1214,  ...,  0.0707, -0.1888, -0.0771],\n",
      "        [ 0.1751, -0.1484, -0.1763,  ..., -0.1771,  0.0013,  0.1413]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Finding the initial weights \n",
    "print(model.layer[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "85d6b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n"
     ]
    }
   ],
   "source": [
    "model = trainMLP(model, X, y, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08254573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1044,  0.0422, -0.1435,  ..., -0.1271,  0.2082,  0.2289],\n",
      "        [ 0.0812,  0.0372,  0.1736,  ..., -0.1752, -0.0095, -0.0008],\n",
      "        [-0.0611,  0.2747, -0.0505,  ...,  0.0721,  0.2734, -0.1974],\n",
      "        ...,\n",
      "        [ 0.0335,  0.2382, -0.0788,  ...,  0.0092, -0.1270, -0.1845],\n",
      "        [ 0.0539,  0.1625, -0.0052,  ...,  0.0854, -0.0151, -0.0970],\n",
      "        [ 0.1674, -0.1501, -0.1517,  ..., -0.2085, -0.0396,  0.2282]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Updates weights to first hidden layer after training the model\n",
    "print(model.layer[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff1d4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evalMLP(model, X, y, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2b582e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008619799806230231\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b779635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Training epoch 200/200\n",
      "Fold 1 - Train loss: 0.0114, Test loss: 0.0309\n",
      "\n",
      "Fold 2/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Fold 2 - Train loss: 0.0152, Test loss: 0.0338\n",
      "\n",
      "Fold 3/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Training epoch 180/200\n",
      "Fold 3 - Train loss: 0.0122, Test loss: 0.0312\n",
      "\n",
      "Fold 4/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Training epoch 160/200\n",
      "Fold 4 - Train loss: 0.0138, Test loss: 0.0335\n",
      "\n",
      "Fold 5/5\n",
      "Training epoch 20/200\n",
      "Training epoch 40/200\n",
      "Training epoch 60/200\n",
      "Training epoch 80/200\n",
      "Training epoch 100/200\n",
      "Training epoch 120/200\n",
      "Training epoch 140/200\n",
      "Fold 5 - Train loss: 0.0134, Test loss: 0.0312\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = crossvalidate(X, y, criterion, optimizer_class, optimizer_kwargs, hidden_layers=[100,200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47cb8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011366265383167345, 0.015161176614961287, 0.012182646164433352, 0.013753961977838982, 0.013384812867355827]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f96013af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03088734111505529, 0.03380079461733751, 0.03119414217046651, 0.033505320619476996, 0.031192460829042097]\n"
     ]
    }
   ],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25305418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
