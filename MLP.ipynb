{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510a5057",
   "metadata": {},
   "source": [
    "This notebook will be the first one for training an MLP. I will create functions for training, testing, crossvalidation and randomsearch to have a seamless analysis. The baseline default value I will use for the initial MLP will come from sklearn's MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2d77d",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "<br />\n",
    "class sklearn.neural_network.MLPRegressor(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49da22a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "pd.set_option(\"max_seq_items\", None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89435633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f672ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('data/train.pkl','rb'))\n",
    "test = pickle.load(open('data/test.pkl','rb'))\n",
    "preprocess_pipeline = pickle.load(open('data/pipeline.pkl', 'rb'))\n",
    "\n",
    "train = preprocess_pipeline.fit_transform(train)\n",
    "test = preprocess_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9eecc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7930, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f233327",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('pressure',axis=1).values\n",
    "y = train['pressure'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d41f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787ac09",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560bd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_layers=None, dropout_prob=0):\n",
    "        \"\"\"\n",
    "        - input_dim: number of neurons on input layer\n",
    "        - hidden_layers: list of integers where each integer represents\n",
    "                         the number of neurons in that hidden layer\n",
    "        (ie. [100, 200]: input_dim -> 100 -> relu -> 200 -> relu -> output)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        hidden_layers = hidden_layers or [100]\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for h in hidden_layers:\n",
    "            layers.append(nn.Linear(in_dim,h))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(p=dropout_prob))    # dropout regularisation\n",
    "            in_dim = h\n",
    "          \n",
    "        layers.append(nn.Linear(in_dim,1))    # output layer\n",
    "        self.layer = nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.layer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500782b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.count = 0\n",
    "\n",
    "    def earlystop(self, val_loss):\n",
    "        # Significant decrease in validation loss\n",
    "        if (self.best_val_loss - val_loss) > self.delta:\n",
    "            self.best_val_loss = val_loss\n",
    "            self.count = 0\n",
    "\n",
    "        # Insignificant decrease in validation loss\n",
    "        elif (self.best_val_loss - val_loss) <= self.delta:\n",
    "            self.count += 1\n",
    "            if self.count > self.patience:\n",
    "                return True\n",
    "            \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b1ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP(model, X, y, criterion, optimizer, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - optimizer: parameter optimizer algorithm\n",
    "    - batch_size: batch size for mini-batch gradient descent\n",
    "    - num_epochs: number of epochs to train\n",
    "    \n",
    "    Output:\n",
    "    - model: trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    train_dataset = TensorDataset(X, y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "            \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backwardprop and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "636426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMLP(model, X, y, criterion, batch_size=32):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - batch_size: data loader batch size (not very important, only lightens the load for loss each calculation)\n",
    "    \n",
    "    Output:\n",
    "    - mean_loss: mean loss from all tested samples\n",
    "    \"\"\"\n",
    "        \n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item() * batch_X.size(0)\n",
    "            total_samples += batch_X.size(0)\n",
    "            \n",
    "    return total_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da270825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMLP_earlystop(model, X, y, criterion, optimizer, patience=5, delta= 0, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - model: predefined model instance\n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss function\n",
    "    - optimizer: parameter optimizer algorithm\n",
    "    - patience: number of consecutive insignificant decreases in validation loss until early stop to activate\n",
    "    - delta: the magnitude a decrease needs to be significant (val_loss new - val_loss old) \n",
    "    - batch_size: batch size for mini-batch gradient descent\n",
    "    - num_epochs: number of epochs to train\n",
    "    \n",
    "    Output:\n",
    "    - model: trained model\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=123)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "    es = EarlyStopper(patience, delta)\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training epochs\"):\n",
    "        model.train()\n",
    "            \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            # .squeeze() to match target shape\n",
    "            outputs = model(batch_X).squeeze()  # [batch_size, 1] => [batch_size]\n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # early stopping check\n",
    "        val_loss = evalMLP(model, X_val, y_val, criterion, batch_size)\n",
    "        if es.earlystop(val_loss):\n",
    "            # Break epoch training loop if earlystop returns True\n",
    "            print(f'Early Stopped at epoch {epoch}')\n",
    "            break\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1656b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate(X, y, criterion, optimizer_class, optimizer_kwargs, patience=15, delta= 0,\n",
    "                  hidden_layers=[100], dropout_prob=0, num_folds=5, batch_size=32, num_epochs=200):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "    - X: features as a tensor\n",
    "    - y: target as a tensor\n",
    "    - criterion: loss functions\n",
    "    - optimizer_class: gradient optimisation\n",
    "    - optimizer_kwargs: learning rate as input\n",
    "    - patience: number of consecutive insignificant decreases in validation loss until early stop to activate\n",
    "    - delta: the magnitude a decrease needs to be significant (val_loss new - val_loss old) \n",
    "    - hidden_layers: list of integers where each integer represents\n",
    "                     the number of neurons in that hidden layer\n",
    "    - dropout_prob: probability of neurons dropping out\n",
    "    - num_folds: number of cross validation folds\n",
    "    - batch_size: batch size for DataLoader (mini-batch gradient descent)\n",
    "    - num_epochs: number of epochs to train per fold\n",
    "    \n",
    "    Output\n",
    "    - fold_train_loss: list of training losses for each fold\n",
    "    - fold_test_loss: list of testing losses for each fold\n",
    "    \"\"\"\n",
    "        \n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=123)\n",
    "    \n",
    "    fold_train_loss = []\n",
    "    fold_test_loss = []\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        # Create new model and optimizer for each fold\n",
    "        model = MLP(X.shape[1], hidden_layers, dropout_prob)\n",
    "        optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Train with current fold's data\n",
    "        model = trainMLP_earlystop(model, X_train, y_train, criterion, optimizer,\n",
    "                                   patience, delta, batch_size, num_epochs)\n",
    "        \n",
    "        # Evaluate on both train and test sets\n",
    "        train_loss = evalMLP(model, X_train, y_train, criterion)\n",
    "        test_loss = evalMLP(model, X_test, y_test, criterion)\n",
    "        \n",
    "        fold_train_loss.append(train_loss)\n",
    "        fold_test_loss.append(test_loss)\n",
    "        \n",
    "        print(f'Fold {fold+1} - Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}')\n",
    "    \n",
    "    return fold_train_loss, fold_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e25c59d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_config(param_dict, random_state=None):\n",
    "    \"\"\"\n",
    "    Samples a hyperparameter configuration from the given search space.\n",
    "    \n",
    "    Parameters:\n",
    "    - param_dict (dict): Dictionary defining the distributions for hyperparameters.\n",
    "        Expected keys and tuple values:\n",
    "          'lr': (min, max) -> log-uniform sample\n",
    "          'weight_decay': (min, max) -> log-uniform sample\n",
    "          'num_layers': (min, max) -> integer uniform sample (inclusive)\n",
    "          'num_neurons': (min, max) -> integer uniform sample for each layer (inclusive)\n",
    "          'dropout_prob': (min, max) -> uniform sample\n",
    "    - random_state (int, optional): Seed for the random number generator to ensure reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - config (dict): A dictionary containing a sampled configuration.\n",
    "      For the neural network architecture, key 'hidden_layers' contains a \n",
    "      list with a random integer (number of neurons) for each hidden layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the random state if provided\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    # Default hyperparameter values\n",
    "    defaults = {\n",
    "        'lr': 1e-2,\n",
    "        'weight_decay': 0.0,\n",
    "        'num_layers': 1,\n",
    "        'num_neurons': 100,\n",
    "        'dropout_prob': 0.0\n",
    "    }\n",
    "    \n",
    "    config = {}\n",
    "    \n",
    "    # Sample learning rate from log-uniform distribution.\n",
    "    if 'lr' in param_dict:\n",
    "        low, high = param_dict['lr']\n",
    "        config['lr'] = np.exp(np.random.uniform(np.log(low), np.log(high)))\n",
    "    else:\n",
    "        config['lr'] = defaults['lr']\n",
    "        \n",
    "    # Sample weight decay from log-uniform distribution.\n",
    "    if 'weight_decay' in param_dict:\n",
    "        low, high = param_dict['weight_decay']\n",
    "        config['weight_decay'] = np.exp(np.random.uniform(np.log(low), np.log(high)))\n",
    "    else:\n",
    "        config['weight_decay'] = defaults['weight_decay']\n",
    "        \n",
    "    # Sample number of layers from an integer uniform distribution.\n",
    "    if 'num_layers' in param_dict:\n",
    "        low, high = param_dict['num_layers']\n",
    "        # np.random.randint is exclusive on the upper bound so add 1.\n",
    "        config['num_layers'] = np.random.randint(low, high + 1)\n",
    "    else:\n",
    "        config['num_layers'] = defaults['num_layers']\n",
    "        \n",
    "    # Sample dropout probability from a uniform distribution.\n",
    "    if 'dropout_prob' in param_dict:\n",
    "        low, high = param_dict['dropout_prob']\n",
    "        config['dropout_prob'] = np.random.uniform(low, high)\n",
    "    else:\n",
    "        config['dropout_prob'] = defaults['dropout_prob']\n",
    "    \n",
    "    # Sample number of neurons for each hidden layer from an integer uniform distribution.\n",
    "    if 'num_neurons' in param_dict:\n",
    "        low, high = param_dict['num_neurons']\n",
    "        n_layers = config.get('num_layers', defaults['num_layers'])\n",
    "        config['hidden_layers'] = [np.random.randint(low, high + 1) for _ in range(n_layers)]\n",
    "    else:\n",
    "        n_layers = config.get('num_layers', defaults['num_layers'])\n",
    "        config['hidden_layers'] = [defaults['num_neurons'] for _ in range(n_layers)]\n",
    "        \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f216e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "############    Sample search space    ############\n",
    "\n",
    "# param_dict = {\n",
    "#     'lr': (1e-5, 1e-1),               # Log-uniform \n",
    "#     'weight_decay': (1e-7, 1e-3),     # Log-uniform \n",
    "#     'num_layers': (1,5),              # Integer uniform\n",
    "#     'num_neurons': (50, 200),         # Integer uniform for each layer\n",
    "#     'dropout_prob': (0.0, 0.5)        # Uniform \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(X, y, param_dict, criterion, optimizer_class, num_trials=400, \n",
    "                  num_folds=5, patience=16, delta=0, batch_size=512, \n",
    "                  num_epochs=2000, random_state=None):\n",
    "    \"\"\"\n",
    "    Perform random search hyperparameter optimization with cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: Features tensor\n",
    "    - y: Target tensor\n",
    "    - param_dict: Hyperparameter search space configuration\n",
    "    - criterion: Loss function\n",
    "    - num_trials: Number of random configurations to try\n",
    "    - num_folds: Number of cross-validation folds\n",
    "    - Other params: Match crossvalidate() parameters\n",
    "    \n",
    "    Returns:\n",
    "    - best_config: Dictionary of best hyperparameters\n",
    "    - best_loss: lowest crossvalidation loss\n",
    "    \"\"\"\n",
    "    best_config = None\n",
    "    best_loss = float('inf')\n",
    "    all_results = []\n",
    "    \n",
    "    for trial in range(num_trials):\n",
    "        print(f\"\\n============   Trial {trial+1}/{num_trials}   ============\")\n",
    "        \n",
    "        # Generate unique seed for each trial if random_state provided\n",
    "        trial_seed = random_state + trial if random_state else None\n",
    "        \n",
    "        # Sample hyperparameter configuration\n",
    "        config = sample_config(param_dict, random_state=trial_seed)\n",
    "        \n",
    "        # Model architecture\n",
    "        model_params = {\n",
    "            'hidden_layers': config['hidden_layers'],\n",
    "            'dropout_prob': config['dropout_prob']\n",
    "        }\n",
    "        # Optimizer parameters\n",
    "        optimizer_params = {\n",
    "            'lr': config['lr'],\n",
    "            'weight_decay': config['weight_decay']\n",
    "        }\n",
    "        \n",
    "        # Cross-validation\n",
    "        fold_train_loss, fold_test_loss = crossvalidate(\n",
    "            X, y, criterion, optimizer_class, optimizer_params,\n",
    "            patience=patience, delta=delta, **model_params,\n",
    "            num_folds=num_folds, batch_size=batch_size, num_epochs=num_epochs\n",
    "        )\n",
    "        test_loss = np.mean(fold_test_loss)\n",
    "        \n",
    "        # Track results\n",
    "        trial_result = {\n",
    "            'config': config,\n",
    "            'avg_test_loss': test_loss,\n",
    "            'fold_test_losses': fold_test_loss,\n",
    "            'fold_train_losses': fold_train_loss\n",
    "        }\n",
    "        all_results.append(trial_result)\n",
    "        \n",
    "        # 7. Update best configuration\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_config = config\n",
    "            print(f\"New best! Loss: {best_loss:.4f} | Config: {config}\")\n",
    "    \n",
    "    return best_config, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef60bbb",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e944d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer): Sequential(\n",
      "    (0): Linear(in_features=21, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0, inplace=False)\n",
      "    (3): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0, inplace=False)\n",
      "    (6): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], [100,200])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28779d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer_class = torch.optim.Adam\n",
    "optimizer_kwargs = {'lr': 0.0001}\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d6b11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:19<00:00,  2.50it/s]\n"
     ]
    }
   ],
   "source": [
    "model = trainMLP(model, X, y, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff1d4feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = evalMLP(model, X, y, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2b582e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008307149163390763\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b779635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:50<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train loss: 0.0114, Test loss: 0.0306\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [00:47<00:19,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train loss: 0.0154, Test loss: 0.0334\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 145/200 [00:42<00:16,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 146\n",
      "Fold 3 - Train loss: 0.0130, Test loss: 0.0316\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 158/200 [00:59<00:15,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 159\n",
      "Fold 4 - Train loss: 0.0142, Test loss: 0.0341\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 197/200 [00:57<00:00,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 198\n",
      "Fold 5 - Train loss: 0.0120, Test loss: 0.0316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = crossvalidate(X, y, criterion, optimizer_class, optimizer_kwargs, hidden_layers=[100,200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47cb8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011359043847393207, 0.015398503876187734, 0.012962180367351105, 0.014221516618919041, 0.012033879446486631]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f96013af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03062584287277898, 0.03340053318866661, 0.03159881535233283, 0.03405488088574945, 0.031592662123088935]\n"
     ]
    }
   ],
   "source": [
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25305418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 4.245876440324545e-05,\n",
       " 'weight_decay': 6.439322405138398e-05,\n",
       " 'num_layers': 5,\n",
       " 'dropout_prob': 0.4182060713012252,\n",
       " 'hidden_layers': [158, 65, 112, 71, 81]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_dict = {\n",
    "    'lr': (1e-5, 1e-1),               # Log-uniform \n",
    "    'weight_decay': (1e-7, 1e-3),     # Log-uniform \n",
    "    'num_layers': (1,5),              # Integer uniform\n",
    "    'num_neurons': (50, 200),         # Integer uniform for each layer\n",
    "    'dropout_prob': (0.0, 0.5)        # Uniform \n",
    "}\n",
    "sample_config(param_dict, random_state=122)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bae61",
   "metadata": {},
   "source": [
    "# RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14843a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1/3\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 34/200 [00:06<00:33,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 35\n",
      "Fold 1 - Train loss: 0.1403, Test loss: 0.1449\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 23/200 [00:05<00:39,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 24\n",
      "Fold 2 - Train loss: 0.1509, Test loss: 0.1465\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16/200 [00:03<00:37,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 17\n",
      "Fold 3 - Train loss: 0.1771, Test loss: 0.1834\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 29/200 [00:06<00:36,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 30\n",
      "Fold 4 - Train loss: 0.1642, Test loss: 0.1654\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 46/200 [00:12<00:41,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 47\n",
      "Fold 5 - Train loss: 0.1184, Test loss: 0.1192\n",
      "New best! Loss: 0.1519 | Config: {'lr': 0.049712909978071915, 'weight_decay': 0.0010000000000000002, 'num_layers': 1, 'dropout_prob': 0.34544242751343085, 'hidden_layers': [67]}\n",
      "\n",
      "Trial 2/3\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:04<00:42,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 22\n",
      "Fold 1 - Train loss: 0.0827, Test loss: 0.0859\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 19/200 [00:05<00:51,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 20\n",
      "Fold 2 - Train loss: 0.0826, Test loss: 0.0843\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:08<01:24,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 19\n",
      "Fold 3 - Train loss: 0.0950, Test loss: 0.0979\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 49/200 [00:14<00:43,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 50\n",
      "Fold 4 - Train loss: 0.0768, Test loss: 0.0811\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [00:07<01:11,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 21\n",
      "Fold 5 - Train loss: 0.0965, Test loss: 0.1015\n",
      "New best! Loss: 0.0901 | Config: {'lr': 0.012766295887411357, 'weight_decay': 0.0010000000000000002, 'num_layers': 1, 'dropout_prob': 0.29516228086591206, 'hidden_layers': [166]}\n",
      "\n",
      "Trial 3/3\n",
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [00:10<01:53,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train loss: 0.2449, Test loss: 0.2456\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 17/200 [00:09<01:46,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 18\n",
      "Fold 2 - Train loss: 0.3681, Test loss: 0.3705\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:07<01:13,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 19\n",
      "Fold 3 - Train loss: 0.2105, Test loss: 0.2255\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [00:13<01:25,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 29\n",
      "Fold 4 - Train loss: 0.2040, Test loss: 0.2058\n",
      "Fold 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:07<01:05,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early Stopped at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train loss: 0.1632, Test loss: 0.1692\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    'lr': (1e-5, 1e-1),               # Log-uniform \n",
    "    'weight_decay': (1e-7, 1e-3),     # Log-uniform \n",
    "    'num_layers': (1,5),              # Integer uniform\n",
    "    'num_neurons': (32, 512),         # Integer uniform for each layer\n",
    "    'dropout_prob': (0.0, 0.5)        # Uniform \n",
    "}\n",
    "\n",
    "# Run random search\n",
    "best_config, best_loss = random_search(\n",
    "    X, y,\n",
    "    param_dict=param_dict,\n",
    "    criterion=nn.MSELoss(),\n",
    "    optimizer_class=torch.optim.AdamW,\n",
    "    num_trials=512,\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f7ed77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
